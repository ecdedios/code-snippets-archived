{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setting the random seed for reproducibility\n",
    "import random\n",
    "random.seed(493)\n",
    "\n",
    "# for manipulating dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for statistical testing\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# natural language processing: n-gram ranking\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# for natural language processing: named entity recognition\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# for working with timestamps\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# for visualizations\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# to print out all the outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a csv file\n",
    "df = pd.read_csv('../data/in/short_survey_data_with_asat_rr_csat.csv')\n",
    "\n",
    "# Read an excel file\n",
    "df = pd.read_excel('../data/in/short_survey_data_with_asat_rr_csat.xlsx')\n",
    "\n",
    "# Read a tsv file\n",
    "df = pd.read_csv('../data/in/short_survey_data_with_asat_rr_csat.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading All Files Within a Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Tarun Dey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/in/short_survey_data_with_asat_rr_csat.csv')\n",
    "\n",
    "def show_missing(df):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and a column and returns a dataframe of\n",
    "    total missing values and percent missing values by column.\n",
    "\n",
    "            Parameters:\n",
    "                    df (dataframe): A pandas dataframe\n",
    "\n",
    "            Returns:\n",
    "                    dfx (dataframe): A list of cleaned words\n",
    "    \"\"\"\n",
    "    null_count = df.isnull().sum()\n",
    "    null_percentage = (null_count / df.shape[0]) * 100\n",
    "    empty_count = pd.Series(((df == ' ') | (df == '')).sum())\n",
    "    empty_percentage = (empty_count / df.shape[0]) * 100\n",
    "    nan_count = pd.Series(((df == 'nan') | (df == 'NaN')).sum())\n",
    "    nan_percentage = (nan_count / df.shape[0]) * 100\n",
    "    dfx = pd.DataFrame({'num_missing': null_count, 'missing_percentage': null_percentage,\n",
    "                         'num_empty': empty_count, 'empty_percentage': empty_percentage,\n",
    "                         'nan_count': nan_count, 'nan_percentage': nan_percentage})\n",
    "    return dfx\n",
    "\n",
    "show_missing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/in/short_survey_data_with_asat_rr_csat.csv')\n",
    "\n",
    "def get_values(df, columns):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and a target column name and prints out the\n",
    "    value counts of the specific column.\n",
    "\n",
    "            Parameters:\n",
    "                    df (dataframe): A pandas dataframe\n",
    "\n",
    "            Returns:\n",
    "                    null (NoneType):\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        print(column)\n",
    "        print('=====================================')\n",
    "        print(df[column].value_counts(dropna=False))\n",
    "        print('\\n')\n",
    "\n",
    "def show_values(df, param):\n",
    "    if param == 'all':\n",
    "        get_values(df, df.columns)\n",
    "    else:\n",
    "        get_values(df, param) \n",
    "\n",
    "show_values(df, ['asat'])\n",
    "show_values(df, 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_STOPWORDS = ['ignore', 'me']\n",
    "\n",
    "def clean(sentence):\n",
    "    \"\"\"\n",
    "    Takes a string and returns a list of cleaned words. All the words that\n",
    "    are not designated as a stop word is then lemmatized after\n",
    "    encoding and basic regex parsing are performed.\n",
    "\n",
    "            Parameters:\n",
    "                    sentence (str): A list of words\n",
    "\n",
    "            Returns:\n",
    "                    word_list (list): A list of cleaned words\n",
    "    \"\"\"\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "    sentence = (unicodedata.normalize('NFKD', sentence)\n",
    "        .encode('ascii', 'ignore')\n",
    "        .decode('utf-8', 'ignore')\n",
    "        .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', sentence).split()\n",
    "    word_list = [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "    return word_list\n",
    "\n",
    "clean('The quick brown fox jumps over the lazy dog. Ignore me.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/in/short_survey_data_with_asat_rr_csat.csv')\n",
    "\n",
    "def get_words(df, column):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and a column and returns a list of\n",
    "    cleaned words that is returned by clean().\n",
    "\n",
    "            Parameters:\n",
    "                    df (dataframe): A pandas dataframe\n",
    "                    column (series): A pandas series\n",
    "\n",
    "            Returns:\n",
    "                    word_list (list): A list of cleaned words\n",
    "    \"\"\"\n",
    "    return clean(''.join(str(df[column].tolist())))\n",
    "\n",
    "get_words(df, 'comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csat =  df.loc[df['score'] == 1]\n",
    "df_dsat =  df.loc[df['score'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csat_words = get_words(df_csat, 'comment')\n",
    "dsat_words = get_words(df_dsat, 'comment')\n",
    "all_words = get_words(df, 'comment')\n",
    "\n",
    "csat_freq = pd.Series(csat_words).value_counts()\n",
    "dsat_freq = pd.Series(dsat_words).value_counts()\n",
    "all_freq = pd.Series(all_words).value_counts()\n",
    "\n",
    "word_counts = (pd.concat([all_freq, csat_freq, dsat_freq], axis=1, sort=True)\n",
    "                .set_axis(['all', 'csat', 'dsat'], axis=1, inplace=False)\n",
    "                .fillna(0)\n",
    "                .apply(lambda s: s.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most frequently occuring words?\n",
    "word_counts.sort_values(by='all', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any words that uniquely identify a dsat or csat comment?\n",
    "pd.concat([word_counts[word_counts.dsat == 0].sort_values(by='csat').tail(10),\n",
    "           word_counts[word_counts.csat == 0].sort_values(by='dsat').tail(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    unigrams with value counts.\n",
    "    \"\"\"\n",
    "    return  pd.Series(words).value_counts()\n",
    "\n",
    "def get_bigrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    bigrams with value counts.\n",
    "    \"\"\"\n",
    "    return (pd.Series(nltk.ngrams(words, 2)).value_counts())[:20]\n",
    "\n",
    "def get_trigrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    trigrams with value counts.\n",
    "    \"\"\"\n",
    "    return (pd.Series(nltk.ngrams(words, 3)).value_counts())[:20]\n",
    "\n",
    "def get_qualgrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    qualgrams with value counts.\n",
    "    \"\"\"\n",
    "    return (pd.Series(nltk.ngrams(words, 4)).value_counts())[:20]\n",
    "\n",
    "def get_ngrams(df,column):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe with column name and generates a\n",
    "    dataframe of unigrams, bigrams, trigrams, and qualgrams.\n",
    "    \"\"\"\n",
    "    return get_bigrams(get_words(df,column)).to_frame().reset_index().rename(columns={'index':'bigram','0':'count'}), \\\n",
    "           get_trigrams(get_words(df,column)).to_frame().reset_index().rename(columns={'index':'trigram','0':'count'}), \\\n",
    "           get_qualgrams(get_words(df,column)).to_frame().reset_index().rename(columns={'index':'qualgram','0':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize N-grams with Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_bigrams(df, column, title):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe, target column name, and specified title\n",
    "    for the bar chart visualization of bigrams.\n",
    "    \"\"\"\n",
    "    get_bigrams(get_words(df,column)).sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Bigram')\n",
    "    plt.xlabel('# Occurances')\n",
    "\n",
    "def viz_trigrams(df, column, title):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe, target column name, and specified title\n",
    "    for the bar chart visualization of trigrams.\n",
    "    \"\"\"\n",
    "    get_trigrams(get_words(df,column)).sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Trigram')\n",
    "    plt.xlabel('# Occurances')\n",
    "    \n",
    "def viz_qualgrams(df, column, title):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe, target column name, and specified title\n",
    "    for the bar chart visualization of qualgrams.\n",
    "    \"\"\"\n",
    "    get_bigrams(get_words(df,column)).sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Qualgram')\n",
    "    plt.xlabel('# Occurances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating DataFrames Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1,df2,df3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Dataframe Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df1.merge(df2,\n",
    "                      left_on='id1',\n",
    "                      right_on='id2',\n",
    "                      suffixes=('_left', '_right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using iLoc to Select Rows of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single selections using iloc and DataFrame\n",
    "\n",
    "# Rows:\n",
    "df.iloc[0] # first row of data frame (Aleshia Tomkiewicz) - Note a Series data type output.\n",
    "df.iloc[1] # second row of data frame (Evan Zigomalas)\n",
    "df.iloc[-1] # last row of data frame (Mi Richan)\n",
    "\n",
    "# Columns:\n",
    "df.iloc[:,0] # first column of data frame (first_name)\n",
    "df.iloc[:,1] # second column of data frame (last_name)\n",
    "df.iloc[:,-1] # last column of data frame (id)\n",
    "\n",
    "# Multiple row and column selections using iloc and DataFrame\n",
    "\n",
    "df.iloc[0:5] # first five rows of dataframe\n",
    "df.iloc[:, 0:2] # first two columns of data frame with all rows\n",
    "df.iloc[[0,3,6,24], [0,5,6]] # 1st, 4th, 7th, 25th row + 1st 6th 7th columns.\n",
    "df.iloc[0:5, 5:8] # first 5 rows and 5th, 6th, 7th columns of data frame (county -> phone1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using loc to Select Rows of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows with first name Ednalyn, include all columns between 'city' and 'email'\n",
    "df.loc[data['first_name'] == 'Ednalyn', 'city':'email']\n",
    " \n",
    "# Select rows where the email column ends with 'gmail.com', include all columns\n",
    "df.loc[data['email'].str.endswith(\"gmail.com\")]   \n",
    " \n",
    "# Select rows with first_name equal to some values, all columns\n",
    "df.loc[data['first_name'].isin(['Ednalyn', 'Ederlyne', 'Edelyn'])]   \n",
    "       \n",
    "# Select rows with first name Ednalyn and gmail email addresses\n",
    "df.loc[data['email'].str.endswith(\"gmail.com\") & (data['first_name'] == 'Ednalyn')] \n",
    " \n",
    "# select rows with id column between 100 and 200, and just return 'zip' and 'web' columns\n",
    "df.loc[(data['id'] > 100) & (df['id'] <= 200), ['zip', 'web']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying the First and Last Rows of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Shape of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying the First Ten Items of a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying Column Names in Bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['column_name', 'column_name', 'column_name', 'column_name', 'column_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handpicking Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df[['column_name',\n",
    "          'column_name',\n",
    "          'column_name',\n",
    "          'column_name',\n",
    "          'column_name'\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'old_name':'new_name',\n",
    "                        'old_name':'new_name',\n",
    "                        'old_name':'new_name',\n",
    "                        'old_name':'new_name',\n",
    "                        'old_name':'new_name'\n",
    "                        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset =\"column_id\", keep = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0','Unnamed: 0.1','score'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('date_time_zone', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resetting the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding DataFrame Column Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col_total'] = df.col1 + df.col2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Column Based on a Conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], \n",
    "        'age': [42, 52, 36, 24, 73], \n",
    "        'preTestScore': [4, 24, 31, 2, 3],\n",
    "        'postTestScore': [25, 94, 57, 62, 70]}\n",
    "df = pd.DataFrame(data, columns = ['name', 'age', 'preTestScore', 'postTestScore'])\n",
    "\n",
    "df['elderly'] = np.where(df['age']>=50, 'yes', 'no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Column Based on Existing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Date':['10/2/2011', '11/2/2011', '12/2/2011', '13/2/2011'], \n",
    "                    'Event':['Music', 'Poetry', 'Theatre', 'Comedy'], \n",
    "                    'Cost':[10000, 5000, 15000, 2000]})\n",
    "\n",
    "df['Discounted_Price'] = df.apply(lambda row: row.Cost - \n",
    "                                  (row.Cost * 0.1), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Non-null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['column_name'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Rows Where a Column is Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['col_name'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Rows Where Column is in List of Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['col_name'].isin(list_of_values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Rows Where Column is Not in List of Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['col_name'].isin(list_of_values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Rows Based on Multiple Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['column_name'] >= A) & (df['column_name'] <= B)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.col_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Counts Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.concat([df.rating.value_counts(),\n",
    "                    df.rating.value_counts(normalize=True)], axis=1)\n",
    "labels.columns = ['n', 'percent']\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a List from Value Counts Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_index_values = df.col_name.value_counts(dropna=False).index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a DataFrame from Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.column_name.value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info and Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "df.timestamp[:1]\n",
    "\n",
    "dtz = []\n",
    "for ts in df.timestamp:\n",
    "  dtz.append(parse(ts))\n",
    "dtz[:10]\n",
    "\n",
    "df['date_time_zone'] = df.apply(lambda row: parse(row.timestamp), axis=1)\n",
    "\n",
    "df.set_index('date_time_zone', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designating CSAT vs DSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a new column and designates a row as either high or low\n",
    "df['csat'] = np.where(df['rating']>=3, 'high', 'low')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting CSAT and DSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive =  df.loc[df['column_name'] == 'positive']\n",
    "df_negative =  df.loc[df['column_name'] == 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default Pearson for continuous variables\n",
    "corr_matrix = df_totals.corr(method ='pearson')\n",
    "\n",
    "# Use Spearman for ordinal variables\n",
    "corr_matrix = df_totals.corr(method ='spearman')\n",
    "\n",
    "# Setup\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "# vmin and vmax control the range of the colormap\n",
    "sns.heatmap(corr_matrix, cmap='RdBu', annot=True, fmt='.2f',\n",
    "           vmin=-1, vmax=1)\n",
    "\n",
    "plt.title(\"Correlations Between Something and Somethings\")\n",
    "\n",
    "# Add tight_layout to ensure the labels don't get cut off\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violin Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = sns.violinplot(x=\"category\", y=\"numeric\", data=df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"category1\", y=\"numeric_measure\",\n",
    "                hue=\"binary_category\", col=\"category2\",\n",
    "                data=df, kind=\"violin\", split=True,\n",
    "                height=6, aspect=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levene's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.levene(df_group1['col_name'], df_group2['col_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levene's Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levene_hom = []\n",
    "levene_het = []\n",
    "\n",
    "for column in columns_list:\n",
    "    \n",
    "    result = stats.levene(df_group1[column], df_group2[column])[1]\n",
    "    \n",
    "    if result > ALPHA:\n",
    "        interpretation = 'insignificant - HOMOGENOUS'\n",
    "        levene_hom.append(column)\n",
    "    else:\n",
    "        interpretation = 'significant - HETEROGENOUS'\n",
    "        levene_het.append(column)\n",
    "        \n",
    "    print(result, '-', column, ' - ', interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapiro Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = df_group1['col_name'] - df_group2['col_name']\n",
    "stats.shapiro(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mann-Whitney U Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05\n",
    "\n",
    "stat, p = mannwhitneyu(df_group1[column], df_group2[column], df_group3[column])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Same distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Different distribution (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mann-Whitney U Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05\n",
    "\n",
    "mannwhitneyu_same = []\n",
    "mannwhitneyu_diff = []\n",
    "\n",
    "for column in columns_list:\n",
    "    stat, p = mannwhitneyu(df_group1[column], df_group2[column], df_group3[column])\n",
    "    \n",
    "    if p > ALPHA:\n",
    "        interpretation = 'SAME (fail to reject H0)'\n",
    "        print('Statistics=%.3f, p=%.3f' % (stat, p) + ' - ' + column + ' - ' + interpretation)\n",
    "        mannwhitneyu_same.append(column)\n",
    "    else:\n",
    "        interpretation = 'DIFFERENT (reject H0)'\n",
    "        print('Statistics=%.3f, p=%.3f' % (stat, p) + ' - ' + column + ' - ' + interpretation)\n",
    "        mannwhitneyu_diff.append(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent T-testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(df_group1['col_name'], df_group2['col_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent T-testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05\n",
    "\n",
    "for column in levene_hom:\n",
    "    \n",
    "    result = stats.ttest_ind(df_group1[column], df_group2[column])[1]\n",
    "    \n",
    "    if result > ALPHA:\n",
    "        interpretation = 'insignificant - SAME'\n",
    "        ttest_same.append(column)\n",
    "    else:\n",
    "        interpretation = 'significant - DIFFERENT'\n",
    "        ttest_diff.append(column)\n",
    "        \n",
    "    print(result, '-', column, ' - ', interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Treemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.treemap(df, path=['case_origin', 'case_tag'], values='score')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
