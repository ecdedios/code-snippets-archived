{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "Filename: name_iteration_keyword.ipynb\n",
    "\n",
    "Author:   Ednalyn C. De Dios\n",
    "Phone:    (210) 236-2685\n",
    "Email:    ednalyn.dedios@gmail.com\n",
    "\n",
    "Created:  January 00, 2020\n",
    "Updated:  January 00, 2020\n",
    "\n",
    "PURPOSE: describe the purpose of this script.\n",
    "\n",
    "PREREQUISITES: list any prerequisites or\n",
    "assumptions here.\n",
    "\n",
    "DON'T FORGET TO:\n",
    "1. Hydrate.\n",
    "2. Sleep.\n",
    "3. Have fun!\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plan of Attack**\n",
    "\n",
    "- [ ] Planning\n",
    "    - [ ] What question(s) need(s) to be addressed?\n",
    "- [ ] Acquisition\n",
    "    - [ ] Read the data\n",
    "    - [ ] Load into a dataframe\n",
    "- [ ] Preparation\n",
    "    - [ ] Clean Up\n",
    "    - [ ] Feature Engineering\n",
    "- [ ] Exploration\n",
    "    - [ ] create visualizations\n",
    "    - [ ] conduct statistical testing\n",
    "    - [ ] get insights from the data\n",
    "- [ ] Modeling\n",
    "    - [ ] split into train/test\n",
    "    - [ ] predict something\n",
    "    - [ ] cross-validate\n",
    "    - [ ] tune hyperparameters\n",
    "- [ ] Delivery\n",
    "- [ ] Maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reading files from the local machine\n",
    "import os\n",
    "\n",
    "# setting the random seed for reproducibility\n",
    "import random\n",
    "random.seed(493)\n",
    "\n",
    "# for manipulating dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for statistical testing\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# for comparing to the p-value\n",
    "ALPHA = 0.05\n",
    "\n",
    "# natural language processing: n-gram ranking\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# add appropriate words that will be ignored in the analysis\n",
    "ADDITIONAL_STOPWORDS = ['campaign']\n",
    "\n",
    "# for natural language processing: named entity recognition\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# for working with timestamps\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# for visualizations\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# to print out all the outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "  \"\"\"\n",
    "  A simple function to clean up the data. All the words that\n",
    "  are not designated as a stop word is then lemmatized after\n",
    "  encoding and basic regex parsing are performed.\n",
    "  \"\"\"\n",
    "  wnl = nltk.stem.WordNetLemmatizer()\n",
    "  stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "  text = (unicodedata.normalize('NFKD', text)\n",
    "    .encode('ascii', 'ignore')\n",
    "    .decode('utf-8', 'ignore')\n",
    "    .lower())\n",
    "  words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "  return [wnl.lemmatize(word) for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_col(df):\n",
    "    \"\"\"\n",
    "    Write or use a previously written function to return the\n",
    "    total missing values and the percent missing values by column.\n",
    "    \"\"\"\n",
    "    null_count = df.isnull().sum()\n",
    "    null_percentage = (null_count / df.shape[0]) * 100\n",
    "    empty_count = pd.Series(((df == ' ') | (df == '')).sum())\n",
    "    empty_percentage = (empty_count / df.shape[0]) * 100\n",
    "    nan_count = pd.Series(((df == 'nan') | (df == 'NaN')).sum())\n",
    "    nan_percentage = (nan_count / df.shape[0]) * 100\n",
    "    return pd.DataFrame({'num_missing': null_count, 'missing_percentage': null_percentage,\n",
    "                         'num_empty': empty_count, 'empty_percentage': empty_percentage,\n",
    "                         'nan_count': nan_count, 'nan_percentage': nan_percentage})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/campaign_nlp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/campaign_nlp.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/campaign_nlp.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read All Files from a Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(folder):\n",
    "    '''\n",
    "    This function reads each the raw data files as dataframes and\n",
    "    combines them into a single data frame.\n",
    "    '''\n",
    "    for i, file_name in enumerate(os.listdir(input_folder)):\n",
    "        try:\n",
    "            # df = pd.read_excel(os.path.join(input_folder, file_name)) # excel\n",
    "            # df = pd.read_csv(os.path.join(input_folder, file_name), sep='\\t') # tsv file\n",
    "            df = pd.read_csv(os.path.join(input_folder, file_name)) # vanilla csv\n",
    "            df['file_name'] = file_name\n",
    "            if i == 0:\n",
    "                final_df = df.copy()\n",
    "            else:\n",
    "                final_df = final_df.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot read file: {file_name}\")\n",
    "            print(str(e))\n",
    "    return final_df\n",
    "\n",
    "folder = 'G:/path/to/data/parent_folder_name'\n",
    "df = read_data(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating DataFrames Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1,df2,df3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging DataFrames Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df1.merge(df2,\n",
    "                      left_on='id1',\n",
    "                      right_on='id2',\n",
    "                      suffixes=('_left', '_right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using iLoc to Select Rows of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single selections using iloc and DataFrame\n",
    "\n",
    "# Rows:\n",
    "data.iloc[0] # first row of data frame (Aleshia Tomkiewicz) - Note a Series data type output.\n",
    "data.iloc[1] # second row of data frame (Evan Zigomalas)\n",
    "data.iloc[-1] # last row of data frame (Mi Richan)\n",
    "\n",
    "# Columns:\n",
    "data.iloc[:,0] # first column of data frame (first_name)\n",
    "data.iloc[:,1] # second column of data frame (last_name)\n",
    "data.iloc[:,-1] # last column of data frame (id)\n",
    "\n",
    "# Multiple row and column selections using iloc and DataFrame\n",
    "\n",
    "data.iloc[0:5] # first five rows of dataframe\n",
    "data.iloc[:, 0:2] # first two columns of data frame with all rows\n",
    "data.iloc[[0,3,6,24], [0,5,6]] # 1st, 4th, 7th, 25th row + 1st 6th 7th columns.\n",
    "data.iloc[0:5, 5:8] # first 5 rows and 5th, 6th, 7th columns of data frame (county -> phone1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using loc to Select Rows of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows with first name Ednalyn, include all columns between 'city' and 'email'\n",
    "data.loc[data['first_name'] == 'Ednalyn', 'city':'email']\n",
    " \n",
    "# Select rows where the email column ends with 'gmail.com', include all columns\n",
    "data.loc[data['email'].str.endswith(\"gmail.com\")]   \n",
    " \n",
    "# Select rows with first_name equal to some values, all columns\n",
    "data.loc[data['first_name'].isin(['Ednalyn', 'Ederlyne', 'Edelyn'])]   \n",
    "       \n",
    "# Select rows with first name Ednalyn and gmail email addresses\n",
    "data.loc[data['email'].str.endswith(\"gmail.com\") & (data['first_name'] == 'Ednalyn')] \n",
    " \n",
    "# select rows with id column between 100 and 200, and just return 'zip' and 'web' columns\n",
    "data.loc[(data['id'] > 100) & (data['id'] <= 200), ['zip', 'web']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying the First and Last Rows of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying the First Ten Items of a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handpicking Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df[['column_name',\n",
    "          '',\n",
    "          '',\n",
    "          '',\n",
    "          ''\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'old_name':'new_name',\n",
    "                        '':'',\n",
    "                        '':'',\n",
    "                        '':'',\n",
    "                        '':''\n",
    "                        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying Column Names in Bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['col1', 'col2', 'col3', 'col4', 'col5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset =\"column_id\", keep = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Non-null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df['column_name'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Rows Where a Column is Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['col_name'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Rows Where Column is in List of Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['col_name'].isin(list_of_values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Rows Where Column is Not in List of Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['col_name'].isin(list_of_values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.col_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Counts Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.concat([df.rating.value_counts(),\n",
    "                    df.rating.value_counts(normalize=True)], axis=1)\n",
    "labels.columns = ['n', 'percent']\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a List from Value Counts Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_index_values = df.col_name.value_counts(dropna=False).index.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.col_name.value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape and Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "len(some_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info and Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "df.timestamp[:1]\n",
    "\n",
    "dtz = []\n",
    "for ts in df.timestamp:\n",
    "  dtz.append(parse(ts))\n",
    "dtz[:10]\n",
    "\n",
    "df['date_time_zone'] = df.apply(lambda row: parse(row.timestamp), axis=1)\n",
    "\n",
    "df.set_index('date_time_zone', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designating CSAT vs DSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a new column and designates a row as either high or low\n",
    "df['csat'] = np.where(df['rating']>=3, 'high', 'low')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting CSAT and DSAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive =  df.loc[df['column_name'] == 'positive']\n",
    "df_negative =  df.loc[df['column_name'] == 'negative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming a DataFrame Column into a List of Clean Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = df.column.tolist()\n",
    "my_words = clean(''.join(str(good_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(df,column):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe and columns and returns a list of\n",
    "    words from the values in the specified column.\n",
    "    \"\"\"\n",
    "    return clean(''.join(str(df[column].tolist())))\n",
    "\n",
    "def get_unigrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    unigrams with value counts.\n",
    "    \"\"\"\n",
    "    return  pd.Series(words).value_counts()\n",
    "\n",
    "def get_bigrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    bigrams with value counts.\n",
    "    \"\"\"\n",
    "    return (pd.Series(nltk.ngrams(words, 2)).value_counts())[:20]\n",
    "\n",
    "def get_trigrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    trigrams with value counts.\n",
    "    \"\"\"\n",
    "    return (pd.Series(nltk.ngrams(words, 3)).value_counts())[:20]\n",
    "\n",
    "def get_qualgrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    qualgrams with value counts.\n",
    "    \"\"\"\n",
    "    return (pd.Series(nltk.ngrams(words, 4)).value_counts())[:20]\n",
    "\n",
    "def get_ngrams(df,column):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe with column name and generates a\n",
    "    dataframe of unigrams, bigrams, trigrams, and qualgrams.\n",
    "    \"\"\"\n",
    "    return get_bigrams(get_words(df,column)).to_frame().reset_index().rename(columns={'index':'bigram','0':'count'}), \\\n",
    "           get_trigrams(get_words(df,column)).to_frame().reset_index().rename(columns={'index':'trigram','0':'count'}), \\\n",
    "           get_qualgrams(get_words(df,column)).to_frame().reset_index().rename(columns={'index':'qualgram','0':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_bigrams(df,column):\n",
    "    get_bigrams(get_words(df,column)).sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "\n",
    "    plt.title('20 Most Frequently Occuring Bigrams')\n",
    "    plt.ylabel('Bigram')\n",
    "    plt.xlabel('# Occurances')\n",
    "\n",
    "    ticks, _ = plt.yticks()\n",
    "    labels = get_bigrams(get_words(df,column)).reset_index()['index'].apply(lambda t: t[0] + ' ' + t[1]).iloc[::-1]\n",
    "    _ = plt.yticks(ticks, labels)\n",
    "\n",
    "def viz_trigrams(df,column):\n",
    "    get_trigrams(get_words(df,column)).sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "\n",
    "    plt.title('20 Most Frequently Occuring Trigrams')\n",
    "    plt.ylabel('Trigram')\n",
    "    plt.xlabel('# Occurances')\n",
    "\n",
    "    ticks, _ = plt.yticks()\n",
    "    labels = get_trigrams(get_words(df,column)).reset_index()['index'].apply(lambda t: t[0] + ' ' + t[1] + ' ' + t[2]).iloc[::-1]\n",
    "    _ = plt.yticks(ticks, labels)\n",
    "    \n",
    "def viz_qualgrams(df,column):\n",
    "    get_bigrams(get_words(df,column)).sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "\n",
    "    plt.title('20 Most Frequently Occuring Qualgrams')\n",
    "    plt.ylabel('Qualgram')\n",
    "    plt.xlabel('# Occurances')\n",
    "\n",
    "    ticks, _ = plt.yticks()\n",
    "    labels = get_qualgrams(get_words(df,column)).reset_index()['index'].apply(lambda t: t[0] + ' ' + t[1] + ' ' + t[2] + ' ' + t[3] ).iloc[::-1]\n",
    "    _ = plt.yticks(ticks, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual Criteria Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list \n",
    "overall_criteria_list =[] \n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if ('term1' in row['column_name'] and 'term2' in row['column_name']):\n",
    "        overall_criteria_list .append([row.column1,\n",
    "                                        row.column2,\n",
    "                                        row.column3,\n",
    "                                        row.column4,\n",
    "                                        row.column5\n",
    "                                        ])\n",
    "        \n",
    "df = pd.DataFrame(overall_criteria_list, columns=[row.column1,\n",
    "                                                row.column2,\n",
    "                                                row.column3,\n",
    "                                                row.column4,\n",
    "                                                row.column5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrubbing PII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrubadub\n",
    "\n",
    "scrub = lambda x: scrubadub.clean(str(x), replace_with='identifier')\n",
    "df['comment'] = df['comment'].apply(scrub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0','Unnamed: 0.1','score'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('date_time_zone', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resetting the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = nlp(' '.join(df.comment.tolist()))\n",
    "len(article.ents)\n",
    "\n",
    "labels = [x.label_ for x in article.ents]\n",
    "Counter(labels)\n",
    "\n",
    "for ent in article.ents:\n",
    "    if ent.label_ == 'ORG':\n",
    "        print('Entity name: ' + ent.text)\n",
    "\n",
    "items = [x.text for x in article.ents]\n",
    "Counter(items).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding DataFrame Column Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col_total'] = df.col1 + df.col2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default Pearson for continuous variables\n",
    "corr_matrix = df_totals.corr(method ='pearson')\n",
    "\n",
    "# Use Spearman for ordinal variables\n",
    "corr_matrix = df_totals.corr(method ='spearman')\n",
    "\n",
    "# Setup\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "# vmin and vmax control the range of the colormap\n",
    "sns.heatmap(corr_matrix, cmap='RdBu', annot=True, fmt='.2f',\n",
    "           vmin=-1, vmax=1)\n",
    "\n",
    "plt.title(\"Correlations Between Something and Somethings\")\n",
    "\n",
    "# Add tight_layout to ensure the labels don't get cut off\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violin Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = sns.violinplot(x=\"col1\", y=\"col2\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = sns.violinplot(x=\"col1\", y=\"col2\", hue=\"col3\", split=True, data=df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"col1\", y=\"col2\",\n",
    "                hue=\"col3\", col=\"col_title\",\n",
    "                data=df, kind=\"violin\", split=True,\n",
    "                height=6, aspect=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levene's Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.levene(df_group1['col_name'], df_group2['col_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levene's Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levene_hom = []\n",
    "levene_het = []\n",
    "\n",
    "for column in columns_list:\n",
    "    \n",
    "    result = stats.levene(df_group1[column], df_group2[column])[1]\n",
    "    \n",
    "    if result > ALPHA:\n",
    "        interpretation = 'insignificant - HOMOGENOUS'\n",
    "        levene_hom.append(column)\n",
    "    else:\n",
    "        interpretation = 'significant - HETEROGENOUS'\n",
    "        levene_het.append(column)\n",
    "        \n",
    "    print(result, '-', column, ' - ', interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shapiro Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = df_group1['col_name'] - df_group2['col_name']\n",
    "stats.shapiro(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mann-Whitney U Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05\n",
    "\n",
    "stat, p = mannwhitneyu(df_group1[column], df_group2[column], df_group3[column])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Same distribution (fail to reject H0)')\n",
    "else:\n",
    "    print('Different distribution (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mann-Whiteney U Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05\n",
    "\n",
    "mannwhitneyu_same = []\n",
    "mannwhitneyu_diff = []\n",
    "\n",
    "for column in columns_list:\n",
    "    stat, p = mannwhitneyu(df_group1[column], df_group2[column], df_group3[column])\n",
    "    \n",
    "    if p > ALPHA:\n",
    "        interpretation = 'SAME (fail to reject H0)'\n",
    "        print('Statistics=%.3f, p=%.3f' % (stat, p) + ' - ' + column + ' - ' + interpretation)\n",
    "        mannwhitneyu_same.append(column)\n",
    "    else:\n",
    "        interpretation = 'DIFFERENT (reject H0)'\n",
    "        print('Statistics=%.3f, p=%.3f' % (stat, p) + ' - ' + column + ' - ' + interpretation)\n",
    "        mannwhitneyu_diff.append(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent T-testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(df_group1['col_name'], df_group2['col_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent T-testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.05\n",
    "\n",
    "for column in levene_hom:\n",
    "    \n",
    "    result = stats.ttest_ind(df_group1[column], df_group2[column])[1]\n",
    "    \n",
    "    if result > ALPHA:\n",
    "        interpretation = 'insignificant - SAME'\n",
    "        ttest_same.append(column)\n",
    "    else:\n",
    "        interpretation = 'significant - DIFFERENT'\n",
    "        ttest_diff.append(column)\n",
    "        \n",
    "    print(result, '-', column, ' - ', interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violin Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "ax = sns.violinplot(x=\"category\", y=\"numeric\", data=df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"category1\", y=\"numeric_measure\",\n",
    "                hue=\"binary_category\", col=\"category2\",\n",
    "                data=df, kind=\"violin\", split=True,\n",
    "                height=6, aspect=1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
