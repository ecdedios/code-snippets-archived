{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Filename: project_name_iteration_keyword.ipynb\n",
    "\n",
    "Author:   Ednalyn C. De Dios\n",
    "Phone:    (210) 236-2685\n",
    "Email:    ednalyn.dedios@gmail.com\n",
    "\n",
    "Created:  January 00, 2020\n",
    "Updated:  January 00, 2020\n",
    "\n",
    "PURPOSE: describe the purpose of this script.\n",
    "\n",
    "PREREQUISITES: list any prerequisites or assumptions here.\n",
    "\n",
    "DON'T FORGET TO:\n",
    "1. Hydrate.\n",
    "2. Sleep.\n",
    "3. Have fun!\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# manipulate dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# natural language processing: n-gram ranking\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# for natural language processing: named entity recognition\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# for natural language processing: sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "# add appropriate words that will be ignored in the analysis\n",
    "ADDITIONAL_STOPWORDS = ['nan']\n",
    "\n",
    "# for visualizations\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# to print out all the outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read all files from a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(folder):\n",
    "    '''\n",
    "    This function reads each the raw data files as dataframes and\n",
    "    combines them into a single data frame.\n",
    "    '''\n",
    "    for i, file_name in enumerate(os.listdir(input_folder)):\n",
    "        try:\n",
    "            # df = pd.read_excel(os.path.join(input_folder, file_name)) # excel\n",
    "            # df = pd.read_csv(os.path.join(input_folder, file_name), sep='\\t') # tsv file\n",
    "            df = pd.read_csv(os.path.join(input_folder, file_name)) # vanilla csv\n",
    "            df['file_name'] = file_name\n",
    "            if i == 0:\n",
    "                final_df = df.copy()\n",
    "            else:\n",
    "                final_df = final_df.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot read file: {file_name}\")\n",
    "            print(str(e))\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'G:/path/to/data/parent_folder_name'\n",
    "df = read_data(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_values(df, columns):\n",
    "    \"\"\"\n",
    "    Returns value counts of the specified columns.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        print(str(col).upper())\n",
    "        print('==================================================')\n",
    "        print(df[col].value_counts(dropna=False))\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_values(df, ['column', 'column', 'column'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "  \"\"\"\n",
    "  A simple function to clean up the data. All the words that\n",
    "  are not designated as a stop word is then lemmatized after\n",
    "  encoding and basic regex parsing are performed.\n",
    "  \"\"\"\n",
    "  wnl = nltk.stem.WordNetLemmatizer()\n",
    "  stopwords = nltk.corpus.stopwords.words('english')+ ADDITIONAL_STOPWORDS\n",
    "  text = (unicodedata.normalize('NFKD', text)\n",
    "    .encode('ascii', 'ignore')\n",
    "    .decode('utf-8', 'ignore')\n",
    "    .lower())\n",
    "  words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "  return [wnl.lemmatize(word) for word in words if word not in stopwords]\n",
    "\n",
    "def get_words(df,column):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe and columns and returns a list of\n",
    "    words from the values in the specified column.\n",
    "    \"\"\"\n",
    "    return clean(''.join(str(df[column].tolist())))\n",
    "\n",
    "def get_unigrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    unigrams with value counts.\n",
    "    \"\"\"\n",
    "    return  pd.Series(words).value_counts()\n",
    "\n",
    "def get_bigrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    bigrams with value counts.\n",
    "    \"\"\"\n",
    "    return (pd.Series(nltk.ngrams(words, 2)).value_counts())[:40]\n",
    "\n",
    "def get_trigrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    trigrams with value counts.\n",
    "    \"\"\"\n",
    "    return (pd.Series(nltk.ngrams(words, 3)).value_counts())[:40]\n",
    "\n",
    "def get_qualgrams(words):\n",
    "    \"\"\"\n",
    "    Takes in a list of words and returns a series of\n",
    "    qualgrams with value counts.\n",
    "    \"\"\"\n",
    "    return (pd.Series(nltk.ngrams(words, 4)).value_counts())[:40]\n",
    "\n",
    "def get_ngrams(df,column):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe with column name and generates a\n",
    "    dataframe of unigrams, bigrams, trigrams, and qualgrams.\n",
    "    \"\"\"\n",
    "    return get_bigrams(get_words(df,column)).to_frame().reset_index().rename(columns={'index':'bigram','0':'count'}), \\\n",
    "           get_trigrams(get_words(df,column)).to_frame().reset_index().rename(columns={'index':'trigram','0':'count'}), \\\n",
    "           get_qualgrams(get_words(df,column)).to_frame().reset_index().rename(columns={'index':'qualgram','0':'count'})\n",
    "\n",
    "def viz_bigrams(df,column):\n",
    "    get_bigrams(get_words(df,column)).sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "\n",
    "    plt.title('40 Most Frequently Occuring Bigrams')\n",
    "    plt.ylabel('Bigram')\n",
    "    plt.xlabel('# Occurances')\n",
    "\n",
    "def viz_trigrams(df,column):\n",
    "    get_trigrams(get_words(df,column)).sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "\n",
    "    plt.title('40 Most Frequently Occuring Trigrams')\n",
    "    plt.ylabel('Trigram')\n",
    "    plt.xlabel('# Occurances')\n",
    "    \n",
    "def viz_qualgrams(df,column):\n",
    "    get_bigrams(get_words(df,column)).sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "\n",
    "    plt.title('40 Most Frequently Occuring Qualgrams')\n",
    "    plt.ylabel('Qualgram')\n",
    "    plt.xlabel('# Occurances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dataframe from value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_value_counts = df.column.value_counts(dropna=False)[:20].to_frame().reset_index().rename(columns={'index':'column', 'column':'count'})\n",
    "df_value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize value counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.column.value_counts(dropna=False)[:20].sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "plt.title('Top 20 XXX')\n",
    "plt.ylabel('XXX')\n",
    "plt.xlabel('# Occurances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List top 10 values of a column based on another column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_call = df.loc[(df['category_column'] == 'call') | (df['category_column'] == 'Call')]\n",
    "df_chat = df.loc[(df['category_column'] == 'chat') | (df['category_column'] == 'Chat')]\n",
    "df_email = df.loc[(df['category_column'] == 'email') | (df['category_column'] == 'Email')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_top10(col_name, df_names):\n",
    "    for df in df_names:\n",
    "        print(df[col_name].value_counts(dropna=False)[:10].to_frame().reset_index().rename(columns={'index':col_name, col_name:'count'}))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_top10('column_name', [df_call, df_chat, df_email])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize top 10 values of a column based on another column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_top10(col_name, df_names):\n",
    "    for df in df_names:\n",
    "        df.column_name.value_counts(dropna=False)[:10].sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "        plt.title('Top 10 ' + col_name.upper()+'S')\n",
    "        plt.ylabel(col_name)\n",
    "        plt.xlabel('# Occurances')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_top10('column_name', [df_call, df_chat, df_email])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_call.shape[0]\n",
    "df_chat.shape[0]\n",
    "df_email.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
